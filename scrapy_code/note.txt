##创建scrapy项目
	命令行scrapy startproject  [项目名称]
	cd进入项目目录，scrapy  genspider	[爬虫文件名]['域名']，
			若是crawl爬虫，scrapy genspider -t crawl [爬虫文件名] ['域名']，
	pycharm打开项目，项目根目录创建run.py 用于调用该爬虫：
					from  scrapy import cmdline
							cmdline.execute("scrapy crawl [爬虫文件名]".split())

## 
	1、response是一个对象，可以执行xpath和css语法提取数据
	2、提取的数据是个“Selector”或者是“SelectorList”对象。
	    如果想要获取其中的字符串，要执行“getall()”或者“get()”方法
	3、getall():获取Selector中的所有文本，返回列表
	4、get():获取Selector中的第一个文本，返回str
	5、数据解析完成后，要交给pipline处理，可以使用yield返回。或是收集所有item，最后统一使用return返回
	6、pipline:是专门用来保存数据的。
	      * open_spider(self,spider):当爬虫被打开时执行
	      * process_item(self,item,spider): 当爬虫有item传来时会被调用
	      * close_spider(self,spider):当爬虫被关闭时调用
	      要激活pipline，应该在'settings.py'中，设置'ITEM_PIPLINE':
	            ITEM_PIPELINES = {
	                'qsbk.pipelines.QsbkPipeline': 300,
	}                                          
	# 此处300为优先级，当有多个pipline时，优先调用数字小的


## JsonItemExporter和JsonLinesItemExporter
	保存数据时，可以使用这两个类。 
	1、JsonItemExporter：每次把数据添加到内存中，最后统一写入磁盘中。
	    好处：存储的数据格式是一个满足json规则的数据。
	    坏处：如果数据量较大，会比较耗内存。
	2、JsonLinesItemExporter：每次调用export_item()时，就把这个item存储到硬盘中。
	    好处：每次处理数据是时就直接存储到硬盘中，不会耗费内存，数据较为安全。
	    坏处：存储的整个文件不是一个满足json格式的文件。

###CrawSpider:
	需要使用"LinkExtractor"和"Rule"。这两个东西决定爬虫的具体走向。
	1、all设置规则的方法：要能够限制在我们想要的url上，不要跟其他url产生相同的正则表达式即可。
	2、follow设置：如果爬取的页面需要将满足条件的url在进行跟进，那么久设置为True否则为False。
	3、callback设置：如果url对应的页面镇只是为了获取更多的url，并不需要里面的数据，那么可以不指定callback，
										若要里面的数据，则指定callback。

### Scrapy Shell
		可以方便我们做一些数据提取的测试代码。
		命令行输入scrapy shell进入环境

##Request对象
		url：请求的地址
		callback：请求完成后的回调函数
		method：请求方法。get，post
		headers：请求头
		meta：用于不同请求间传递参数
		encoding：编码，默认utf-8
		dot_filter：表示不由调度器过滤。在执行重复请求时要设置。
		errback：错误时执行的函数

##Response对象
		meta：从其他请求传过来的meta属性，保持多个请求间的数据连接
		encoding
		text：返回来的数据作为unicode字符串返回
		body：返回数据作为bytes字符串返回
		xpath
		css


## 发送post请求（模拟登陆）
		1、使用scrapy.FormRequest方法，可以方便的指定表单数据。
		2、如果想在爬虫一开始就发送psot请求，应重写start_requests方法，在这个方法中，发送post请求。


##随机请求头设置
		在middlewares.py文件中新建一个class类，代码如下：
			class UserAgentDownloadMiddleware(object):
					User_Agent= [
						'请求头1号','请求头2号','请求头3号','请求头4号'
					]
				    def process_request(self,request,spider):
					        user_agent = random.choice(self.User_Agent)
					        request.headers['User-Agent'] = user_agent
			在settings.py中设置中间件文件：
				DOWNLOADER_MIDDLEWARES = {
					'headers.middlewares.UserAgentDownloadMiddleware': 543,
																				}
			把原本的中间件改成我们新建的类名

## 代理ip设置
		
				在middlewares.py文件中新建一个class类，代码如下：
						class IpProxyDownloadMiddleWares(object):
						  	  IpProxy = ['http://125.108.71.25:9999',
						               'http://123.149.137.163:9999',
						               "http://144.123.71.122:9999",
						               'http://123.149.137.96:9999']
						    def process_request(self, request, spider):
						        proxy = random.choice(self.IpProxy)
						        request.meta['proxy'] = proxy
			在settings.py中设置中间件文件：
				DOWNLOADER_MIDDLEWARES = {   								
									'headers.middlewares.IpProxyDownloadMiddleWares': 543,
									}
			把原本的中间件改成我们新建的类名


## scrapy框架文件下载  Files Pipeline  &   Img_Pipeline
		一、File Pipeline:
						1、先定义一个Item，然后在Item定义两个属性，file_urls和 files。
						2、file_urls存储需要下载的文件url链接，是一个列表类型。
						3、当文件下载完成后，会把文件下载的相关信息存储到item的files的相关属性中。比如下载路径、下载的ur和文件的校验码等。(用的较少)
						4、在配置文件settings.py中配置FILES_STORE，这个配置文件是用来设置下载下来的存储路径。
						5、settings.py文件中，在ITEM_PIPELINES中设置
							ITEM_PIPELINES = {
									   'scrapy.pipelines.images.ImagesPipeline': 1,	}
							来启动pipline。
		二、Img_Pipeline
						使用过程同Flie Pipeline，只需要将前者中的所有File和file换成Image和image即可。

						下载下来的路径默认是放在一个IMAGES_STORE下的一个full文件夹下，若要分类，需要重写ImagesPipeline中的方法